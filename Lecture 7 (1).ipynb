{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7: Convolutional Neural Networks\n",
    "In this lecture we will discuss the current state of the art network architecture for all things computer vision: the convolutional layer. We'll start by going over the basics of what a convolution is, then discuss the building blocks of a convolutional network, take a look at some applications, and finally train a hotdog detector with transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spicynet](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mlp_mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that neural networks are fairly decent at tasks like MNIST and CIFAR, however, the input size of both these datasets was quite small. CIFAR, for example, has images of size 32x32x3 (3072 total features). In a dense layer, each neuron is connected to all incoming features, this means the input layer of a CIFAR network must have 3072 weights per neuron. Consider a higher resolution image, 200x200x3 for example. This would require 120,000 weights per neuron!\n",
    "\n",
    "Such a huge number of trainable parameters is problematic due to overfitting. It's clear that dense layers aren't well suited to higher resolution images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![doggo](https://memeguy.com/photos/images/my-dog-used-to-chase-people-on-a-bike-a-lot-it-got-so-bad-finally-i-had-to-take-his-bike-away-209332.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense layers connect all incoming features to all neurons, implying that all the features have some relationship to eachother. This is true for many types of problem, but not images! Above, we see a dog along with a bunch of other stuff. If our goal is to figure out a dog is in this picture, the grill behind him doesnt really matter. All that matters are the things that clearly make him a dog, like his cute face.\n",
    "\n",
    "Rather than have each neuron look at every pixel, it makes more sense to only look at a neighborhood of pixels since images tend to have local information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](https://devblogs.nvidia.com/wp-content/uploads/2015/11/Convolution_schematic.gif)\n",
    "![convgif](https://ujwlkarn.files.wordpress.com/2016/08/giphy.gif?w=748)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a convolutional layer, neurons are replaced by __kernels__. Kernels are nxn matrices that slide across an incoming image to produce a similarly shaped output\n",
    "\n",
    "![kernel](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/assets/Conv1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kernels](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-05-at-11-03-00-pm.png?w=342&h=562)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Layers in a ConvNet\n",
    "Other than the convolutional layer itself, image processing networks almost always contain pooling layers. A pooling layer simply reduces the dimension of incoming data. This allows an increase in the number of features between layers without increasing the total computational workload.\n",
    "\n",
    "![pooling](http://cs231n.github.io/assets/cnn/maxpool.jpeg)\n",
    "\n",
    "The intuition behind this is that convolutional kernels are trying to find whether a certain feature is present in an image. We dont care too much about low numbers because they indicate that feature is not present. It's reasonable to simply throw out all but the most promising regions\n",
    "\n",
    "Just as in dense networks, convolutional layers also need an activation, these activations exactly mirror the dense case and ReLU is very prominent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My first convolutional network\n",
    "Let's take a look at implementing and training a convolutional network in MXNet. We'll start by implementing an architecture called Alexnet. This was the architecture that kicked off the deep learning boom, one of the first convolutional networks ever made!\n",
    "\n",
    "Although Alexnet was made for higher resolution images, let's just try it out on CIFAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "# import matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im using gpu to speed things up a little, cpu would be fine though\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noise to training data\n",
    "def train_transformer(data, label):\n",
    "    # make sure all data is the same shape\n",
    "    data = mx.image.imresize(data, 32, 32)\n",
    "    # Change the order of dimensions to (batch, channel, height, width) I dont know why this isnt default\n",
    "    data = mx.nd.transpose(data, (2,0,1))\n",
    "    # convert from int to float\n",
    "    data = data.astype(np.float32)\n",
    "    # normalize the data\n",
    "    data = (data - nd.min(data)) / (nd.max(data) - nd.min(data))\n",
    "    # add some noise for better performance\n",
    "    data = data + .01*nd.random.normal(shape=data.shape)\n",
    "    # force noisy data between 0 and 1\n",
    "    data = nd.clip(data=data, a_min=0, a_max=1)\n",
    "    return data, label\n",
    "\n",
    "# dont add noise to testing data\n",
    "def test_transformer(data, label):\n",
    "    data = mx.image.imresize(data, 32, 32)\n",
    "    data = mx.nd.transpose(data, (2,0,1))\n",
    "    data = data.astype(np.float32)\n",
    "    data = (data - nd.min(data)) / (nd.max(data) - nd.min(data))\n",
    "    return data, label\n",
    "\n",
    "batch_size = 64\n",
    "# note doing this [d for d in dataset] thing helps a lot with speed, i think this is an MXNet bug\n",
    "train_data = gluon.data.DataLoader([d for d in gluon.data.vision.CIFAR10('./data', train=True, transform=train_transformer)],\n",
    "                                    batch_size=batch_size, shuffle=True, last_batch='discard')\n",
    "\n",
    "test_data = gluon.data.DataLoader([d for d in gluon.data.vision.CIFAR10('./data', train=False, transform=test_transformer)],\n",
    "                                   batch_size=batch_size, shuffle=False, last_batch='discard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alexnet](http://cv-tricks.com/wp-content/uploads/2017/03/xalexnet_small-1.png.pagespeed.ic.q5Lnn1-u6h.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_net = gluon.nn.Sequential()\n",
    "with alex_net.name_scope():\n",
    "    #  First convolutional layer\n",
    "    alex_net.add(gluon.nn.Conv2D(channels=96, kernel_size=11, strides=(4,4), padding=5, activation='relu'))\n",
    "    alex_net.add(gluon.nn.MaxPool2D(pool_size=3, padding=1, strides=2))\n",
    "    #  Second convolutional layer\n",
    "    alex_net.add(gluon.nn.Conv2D(channels=192, kernel_size=5, padding=2, activation='relu'))\n",
    "    alex_net.add(gluon.nn.MaxPool2D(pool_size=3, padding=1, strides=(2,2)))\n",
    "    # Third convolutional layer\n",
    "    alex_net.add(gluon.nn.Conv2D(channels=384, kernel_size=3, padding=1, activation='relu'))\n",
    "    # Fourth convolutional layer\n",
    "    alex_net.add(gluon.nn.Conv2D(channels=384, kernel_size=3, padding=1, activation='relu'))\n",
    "    # Fifth convolutional layer\n",
    "    alex_net.add(gluon.nn.Conv2D(channels=256, kernel_size=3, padding=1, activation='relu'))\n",
    "    alex_net.add(gluon.nn.MaxPool2D(pool_size=3, padding=1, strides=2))\n",
    "    # Flatten and apply fullly connected layers\n",
    "    alex_net.add(gluon.nn.Flatten())\n",
    "    alex_net.add(gluon.nn.Dense(4096, activation=\"relu\"))\n",
    "    alex_net.add(gluon.nn.Dense(4096, activation=\"relu\"))\n",
    "    alex_net.add(gluon.nn.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alexnet](http://cv-tricks.com/wp-content/uploads/2017/03/xalexnet_small-1.png.pagespeed.ic.q5Lnn1-u6h.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters, create a trainer, and loss just like usual\n",
    "alex_net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "alex_trainer = gluon.Trainer(alex_net.collect_params(), 'sgd', {'learning_rate': .1})\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating accuracy, note it is identical to dense neural networks\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for d, l in data_iterator:\n",
    "        data = d.as_in_context(ctx)\n",
    "        label = l.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function also is virtually unchanged!\n",
    "def train(net, trainer):\n",
    "    epochs = 7\n",
    "    smoothing_constant = .01\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for i, (d, l) in enumerate(train_data):\n",
    "            data = d.as_in_context(ctx)\n",
    "            label = l.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "\n",
    "            curr_loss = nd.mean(loss).asscalar()\n",
    "            moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                           else (1 - smoothing_constant) * moving_loss + smoothing_constant * curr_loss)\n",
    "\n",
    "            if i > 0 and i % 200 == 0:\n",
    "                print('Batch %d. Loss: %f' % (i, moving_loss))\n",
    "\n",
    "        test_accuracy = evaluate_accuracy(test_data, net)\n",
    "        train_accuracy = evaluate_accuracy(train_data, net)\n",
    "        print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(alex_net, alex_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alexnet is a very outdated architecture and there are many modern architectures that vastly outperform it. One of the most famous is VGG.\n",
    "\n",
    "![vgg](http://www.cs.toronto.edu/~frossard/post/vgg16/vgg16.png)\n",
    "\n",
    "VGG is known for its high accuracy, but also its high computational cost. Let's implement the above image in MXNet and see how it does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "\n",
    "def vgg_block(num_convs, channels):\n",
    "    out = nn.HybridSequential()\n",
    "    for _ in range(num_convs):\n",
    "        out.add(nn.Conv2D(channels=channels, kernel_size=3,\n",
    "                      padding=1, activation='relu'))\n",
    "    out.add(nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    return out\n",
    "\n",
    "def vgg_stack(architecture):\n",
    "    out = nn.HybridSequential()\n",
    "    for (num_convs, channels) in architecture:\n",
    "        out.add(vgg_block(num_convs, channels))\n",
    "    return out\n",
    "\n",
    "num_outputs = 10\n",
    "architecture = ((1,64), (1,128), (1,256), (3,512))\n",
    "vgg = nn.HybridSequential()\n",
    "with vgg.name_scope():\n",
    "    vgg.add(vgg_stack(architecture))\n",
    "    vgg.add(nn.Flatten())\n",
    "    vgg.add(nn.Dense(512, activation=\"relu\"))\n",
    "    vgg.add(nn.Dropout(.5))\n",
    "    vgg.add(nn.Dense(512, activation=\"relu\"))\n",
    "    vgg.add(nn.Dropout(.5))\n",
    "    vgg.add(nn.Dense(num_outputs))\n",
    "    \n",
    "vgg.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)    \n",
    "vgg.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_trainer = gluon.Trainer(vgg.collect_params(), 'sgd', {'learning_rate': .05})\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(vgg, vgg_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning: Hotdog or Not Hotdog\n",
    "Unfortunately, if we want to move beyond CIFAR and MNIST to high resolution (much more useful) images, we can't train a network from scratch using cpus, it would simply take too long. Modern networks are typically trained using multiple GPUS, which are orders of magnitude faster.\n",
    "\n",
    "Fortunately, there's a dataset called Imagenet that has extremely generalized features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagenet](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Sample-of-Images-from-the-ImageNet-Dataset-used-in-the-ILSVRC-Challenge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagenet is a dataset made of over 1 million images labeled with one of 1000 classes. It is such a huge and varied dataset that when a network is trained on it, it learns very general features. This means that we can repurpose the __pretrained__ weights of a network to whatever kind of task we want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](http://jwfromm.com/GIX513/images/transfer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate this, let's create a network that identifies whether an image has a hotdog in it or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from mxnet.test_utils import download\n",
    "\n",
    "# start by downloading some hotdog training images\n",
    "\n",
    "ctx = [mx.gpu()]\n",
    "dataset_files = {'train': ('not_hotdog_train-e6ef27b4.rec', '0aad7e1f16f5fb109b719a414a867bbee6ef27b4'),\n",
    "                 'validation': ('not_hotdog_validation-c0201740.rec', '723ae5f8a433ed2e2bf729baec6b878ac0201740')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont worry too much about this part, its just parsing MXNets silly image records\n",
    "\n",
    "training_dataset, training_data_hash = dataset_files['train']\n",
    "\n",
    "validation_dataset, validation_data_hash = dataset_files['validation']\n",
    "\n",
    "def verified(file_path, sha1hash):\n",
    "    import hashlib\n",
    "    sha1 = hashlib.sha1()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        while True:\n",
    "            data = f.read(1048576)\n",
    "            if not data:\n",
    "                break\n",
    "            sha1.update(data)\n",
    "    matched = sha1.hexdigest() == sha1hash\n",
    "    if not matched:\n",
    "        logging.warn('Found hash mismatch in file {}, possibly due to incomplete download.'\n",
    "                     .format(file_path))\n",
    "    return matched\n",
    "\n",
    "url_format = 'https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/{}'\n",
    "if not os.path.exists(training_dataset) or not verified(training_dataset, training_data_hash):\n",
    "    logging.info('Downloading training dataset.')\n",
    "    download(url_format.format(training_dataset),\n",
    "             overwrite=True)\n",
    "if not os.path.exists(validation_dataset) or not verified(validation_dataset, validation_data_hash):\n",
    "    logging.info('Downloading validation dataset.')\n",
    "    download(url_format.format(validation_dataset),\n",
    "             overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "train_iter = mx.io.ImageRecordIter(path_imgrec=training_dataset,\n",
    "                                   min_img_size=256,\n",
    "                                   data_shape=(3, 224, 224),\n",
    "                                   rand_crop=True,\n",
    "                                   shuffle=True,\n",
    "                                   batch_size=batch_size,\n",
    "                                   max_random_scale=1.5,\n",
    "                                   min_random_scale=0.75,\n",
    "                                   rand_mirror=True)\n",
    "val_iter = mx.io.ImageRecordIter(path_imgrec=validation_dataset,\n",
    "                                 min_img_size=256,\n",
    "                                 data_shape=(3, 224, 224),\n",
    "                                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take a look at some examples\n",
    "for i, batch in enumerate(val_iter):\n",
    "    d = batch.data[0]\n",
    "    l = batch.label[0]\n",
    "    data = d[0]\n",
    "    label = l[0]\n",
    "    data = mx.nd.transpose(data, (1,2,0))\n",
    "    plt.imshow(data.astype(np.uint8).asnumpy())\n",
    "    plt.show()\n",
    "    if label == 0:\n",
    "        print(\"Not a hotdog\")\n",
    "    else:\n",
    "        print(\"Hotdog\")\n",
    "    if i == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.model_zoo import vision as models\n",
    "# lets use a pretrained squeezenet, this a model known for being decently good accuracy at a low computational cost\n",
    "squeezenet = models.squeezenet1_1(pretrained=True, prefix=\"dog_\", ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](https://arxiv.org/pdf/1602.07360.pdf) for more info about squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new copy of squeezenet, this time though only have 2 output classes (hotdog or not hotdog)\n",
    "dognet = models.squeezenet1_1(classes=2, prefix=\"dog_\")\n",
    "dognet.collect_params().initialize(ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dognet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the the features chunk of squeezenet, only leave the output untouched\n",
    "dognet.features = squeezenet.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the trainer, specify that we only want to update the output chunk of dognet\n",
    "trainer = gluon.Trainer(dognet.output.collect_params(), 'sgd', {'learning_rate': .01})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given guess z and label y, compute the loss\n",
    "def unbalanced_loss(loss_func, z, y):\n",
    "    # there are 5 times more images of not hotdogs than hotdogs :(\n",
    "    positive_class_weight = 5\n",
    "    regular_loss = loss_func(z, y)\n",
    "    # convienently y is either 1 (hotdog) or 0 (not hotdog) so scaling is pretty simple\n",
    "    scaled_loss = regular_loss * (1 + y*positive_class_weight)/positive_class_weight\n",
    "    return scaled_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return metrics string representation\n",
    "def metric_str(names, accs):\n",
    "    return ', '.join(['%s=%f'%(name, acc) for name, acc in zip(names, accs)])\n",
    "metric = mx.metric.create(['acc', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.image import color_normalize\n",
    "\n",
    "def evaluate(net, data_iter, ctx):\n",
    "    data_iter.reset()\n",
    "    for batch in data_iter:\n",
    "        data = color_normalize(batch.data[0]/255,\n",
    "                               mean=mx.nd.array([0.485, 0.456, 0.406]).reshape((1,3,1,1)),\n",
    "                               std=mx.nd.array([0.229, 0.224, 0.225]).reshape((1,3,1,1)))\n",
    "        data = gluon.utils.split_and_load(data, ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = []\n",
    "        for x in data:\n",
    "            outputs.append(net(x))\n",
    "        metric.update(label, outputs)\n",
    "    out = metric.get()\n",
    "    metric.reset()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now lets train dognet, this will look very similar to other training loops we've done\n",
    "epochs = 10\n",
    "best_f1 = 0\n",
    "log_interval = 100\n",
    "val_names, val_accs = evaluate(dognet, val_iter, ctx)\n",
    "print('[Initial] validation: %s'%(metric_str(val_names, val_accs)))\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_iter.reset()\n",
    "    btic = time.time()\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        # the model zoo models expect normalized images\n",
    "        data = color_normalize(batch.data[0]/255,\n",
    "                               mean=mx.nd.array([0.485, 0.456, 0.406]).reshape((1,3,1,1)),\n",
    "                               std=mx.nd.array([0.229, 0.224, 0.225]).reshape((1,3,1,1)))\n",
    "        data = gluon.utils.split_and_load(data, ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = []\n",
    "        Ls = []\n",
    "        with autograd.record():\n",
    "            for x, y in zip(data, label):\n",
    "                z = dognet(x)\n",
    "                # rescale the loss based on class to counter the imbalance problem                \n",
    "                L = unbalanced_loss(loss, z, y)\n",
    "                # store the loss and do backward after we have done forward\n",
    "                # on all GPUs for better speed on multiple GPUs.\n",
    "                Ls.append(L)\n",
    "                outputs.append(z)\n",
    "            for L in Ls:\n",
    "                L.backward()\n",
    "        trainer.step(batch.data[0].shape[0])\n",
    "        metric.update(label, outputs)\n",
    "        if log_interval and not (i+1)%log_interval:\n",
    "            names, accs = metric.get()\n",
    "            print('[Epoch %d Batch %d] speed: %f samples/s, training: %s'%(\n",
    "                           epoch, i, batch_size/(time.time()-btic), metric_str(names, accs)))\n",
    "        btic = time.time()\n",
    "\n",
    "    names, accs = metric.get()\n",
    "    metric.reset()\n",
    "    print('[Epoch %d] training: %s'%(epoch, metric_str(names, accs)))\n",
    "    print('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n",
    "    val_names, val_accs = evaluate(dognet, val_iter, ctx)\n",
    "    print('[Epoch %d] validation: %s'%(epoch, metric_str(val_names, val_accs)))\n",
    "\n",
    "    if val_accs[1] > best_f1:\n",
    "        best_f1 = val_accs[1]\n",
    "        print('Best validation f1 found. Checkpointing...')\n",
    "        dognet.save_params('dog-%d.params'%(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgba2rgb\n",
    "import skimage.io as io\n",
    "def classify_hotdog(net, url):\n",
    "    I = io.imread(url)\n",
    "    if I.shape[2] == 4:\n",
    "        I = rgba2rgb(I)\n",
    "    image = mx.nd.array(I).astype(np.uint8)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image.asnumpy())\n",
    "    image = mx.image.resize_short(image, 256)\n",
    "    image, _ = mx.image.center_crop(image, (224, 224))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(image.asnumpy())\n",
    "    image = mx.image.color_normalize(image.astype(np.float32)/255,\n",
    "                                     mean=mx.nd.array([0.485, 0.456, 0.406]),\n",
    "                                     std=mx.nd.array([0.229, 0.224, 0.225]))\n",
    "    image = mx.nd.transpose(image.astype('float32'), (2,1,0))\n",
    "    image = mx.nd.expand_dims(image, axis=0)\n",
    "    out = mx.nd.SoftmaxActivation(net(image))\n",
    "    print('Probabilities are: '+str(out[0].asnumpy()))\n",
    "    result = np.argmax(out.asnumpy())\n",
    "    outstring = ['Not hotdog!', 'Hotdog!']\n",
    "    print(outstring[result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dognet.collect_params().reset_ctx(mx.cpu())\n",
    "classify_hotdog(dognet, \"http://del.h-cdn.co/assets/17/25/980x490/landscape-1498074256-delish-blt-dogs-01.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_hotdog(dognet, \"https://i.ytimg.com/vi/SfLV8hD7zX4/maxresdefault.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
