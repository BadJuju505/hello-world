{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10: Backpropagation and Gradient Descent\n",
    "In this lecture, we will discuss the machinery behind how training neural nets works. To do this, we will review some calculus, perform backpropagation on a simple node, and build up to a fully complex neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as a quick expansion of what this loss function means, let's right it out ourselves and take a look\n",
    "\n",
    "Let's first look at what the softmax function does. Softmax is defined by the equation\n",
    "\n",
    "$F({Y'}_i) = \\frac{e^{{Y'}_i}}{\\sum_{j=0}^{k} e^{{Y'}_j}}$\n",
    "\n",
    "Although it looks a little funky, it basically forces all the values to in $F({Y'})$ to sum to one and represent the probability of each ${Y'}_i$ being true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def softmax(X):\n",
    "    output = np.empty(X.shape)\n",
    "    for i, entry in enumerate(X):\n",
    "        output[i] = np.exp(entry) / np.sum(np.exp(X))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0320586 , 0.08714432, 0.23688282, 0.64391426])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = softmax(np.asarray([1,2,3,4]))\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the sum of the output is 1 and the larger each input is, the higher the probability that it's the right choice. You can see how this is quite useful when we have to choose which class a network should guess!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at cross entropy loss. Cross entropy is a simple way of measuring how far a guess is from the correct answer. It is defined as:\n",
    "\n",
    "$CE(Y', Y) = -\\sum_i log({Y'}_i) \\times Y_i$\n",
    "\n",
    "Where $Y'$ are the guesses made and $Y$ are the correct labels. The log is used to make the equation easier to differentiate, which we'll take a look at later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CE](http://ml-cheatsheet.readthedocs.io/en/latest/_images/cross_entropy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44018969856119544"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CE(output, labels):\n",
    "    return -np.sum(labels*np.log(output))\n",
    "\n",
    "labels = np.asarray([0,0,0,1])\n",
    "CE(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.4401897]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mxnet\n",
    "from mxnet import gluon, nd\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=True)\n",
    "input = nd.array([1,2,3,4])\n",
    "labels = nd.array([3])\n",
    "loss(input, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that our simple code matches what MXNet is doing when we call the softmax cross entropy loss. Great! Now we know whats happening behind the scenes for the loss. \n",
    "\n",
    "Because cross entropy measures how good a guess is (with lower outputs meaning better guesses) our goal is always to minimize the output of the loss function. Let's take a look at a basic perceptron trying to classify MNIST and see how we can minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard mxnet packages\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "\n",
    "# import matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# set the device we should use for computing, we'll just our cpu for now\n",
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/mxnet/gluon/data/vision.py:118: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  label = np.fromstring(fin.read(), dtype=np.uint8).astype(np.int32)\n",
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/mxnet/gluon/data/vision.py:122: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  data = np.fromstring(fin.read(), dtype=np.uint8)\n"
     ]
    }
   ],
   "source": [
    "# load MNIST, a very simple image classification dataset\n",
    "\n",
    "# set the size of the training set\n",
    "num_examples = 60000\n",
    "# set batch size : how many images should i process at a time?\n",
    "batch_size = 64\n",
    "# set the number of pixels per image (32 x 32)\n",
    "num_inputs = 784\n",
    "# set the number of possible outputs (0 through 9)\n",
    "num_outputs = 10\n",
    "# define a function that scales the image pixels down between 0 and 1\n",
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "# load the training data\n",
    "train_data = mx.gluon.data.DataLoader([d for d in mx.gluon.data.vision.MNIST(train=True, transform=transform)],\n",
    "                                      batch_size, shuffle=True)\n",
    "# load the test data\n",
    "test_data = mx.gluon.data.DataLoader([d for d in mx.gluon.data.vision.MNIST(train=False, transform=transform)],\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a minimal neural network\n",
    "net = gluon.nn.Dense(num_outputs, in_units=784, use_bias=True)\n",
    "# initialize the parameters of the network randomly\n",
    "net.collect_params().initialize(mx.init.Normal(), ctx=model_ctx)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![simplenet](http://jwfromm.com/GIX513/images/simplenet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's forumate what's happening here a little better. Since we have a simple one layer network, we can describe it entirely by the equation:\n",
    "\n",
    "$Y' = WX$\n",
    "\n",
    "Where $Y'$ is the 10 outputs of the network, W is the weight matrix of our neurons, with shape (784x10), and $X$ is one input image of MNIST, with shape (1x784). This is a simple matrix multiplication that represents multiply accumulating the weights of each neuron with the corresponding input pixels.\n",
    "\n",
    "As this is a fairly simple equation, it's clear that our goal is to find the values of W (the neurons weights) that make our guesses $Y'$ as accurate as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to see how good our model is\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    # keep track of the accuracy across our dataset\n",
    "    acc = mx.metric.Accuracy()\n",
    "    # iterate through all the data\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        # move the data and label to the proper device\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        # run the data through the network\n",
    "        output = net(data)\n",
    "        # check what our guess is\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        # compute accuracy and update our running tally\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    # return the accuracy\n",
    "    return acc.get()[1]\n",
    "\n",
    "# define a function to see how good our model is based on loss\n",
    "def evaluate_loss(data_iterator, net):\n",
    "    # keep track of the accuracy across our dataset\n",
    "    lossmetric = mx.metric.Loss()\n",
    "    # iterate through all the data\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        # move the data and label to the proper device\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        # run the data through the network\n",
    "        output = net(data)\n",
    "        # check what our guess is\n",
    "        loss = softmax_cross_entropy(output, label)\n",
    "        # compute accuracy and update our running tally\n",
    "        lossmetric.update(None, loss)\n",
    "    # return the accuracy\n",
    "    return lossmetric.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1029"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(test_data, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the accuracy of our network is currently about equal to just guessing the output. Let's brain storm on how we can improve the weight matrix of our network to make better guesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Method 1: Random Search\n",
    "Let's start by trying something very simple, we'll just randomly assign numbers to $W$ and keep whatever works best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.283894435135567\n",
      "15.636196653414835\n",
      "12.048128410182942\n",
      "13.206123843483219\n",
      "12.839609982239052\n",
      "12.887183442033605\n",
      "14.293710940545486\n",
      "13.375741547928177\n",
      "13.425769245263018\n",
      "18.982629643232638\n",
      "12.048128410182942\n"
     ]
    }
   ],
   "source": [
    "# keep track of the best accuracy we've seen\n",
    "bestloss = np.inf\n",
    "# also keep track of the best weights\n",
    "bestW = None\n",
    "bestb = None\n",
    "# try 1000 different weights\n",
    "for i in range(10):\n",
    "    # generate a random weight tensor\n",
    "    W = nd.random.normal(shape=net.weight.shape)\n",
    "    b = nd.random.normal(shape = net.bias.shape)\n",
    "    # assign those weights to our network\n",
    "    net.weight.set_data(W)\n",
    "    net.bias.set_data(b)\n",
    "    # check the accuracy of the network\n",
    "    loss = evaluate_loss(test_data, net)\n",
    "    print(loss)\n",
    "    # see if this is the best we've done\n",
    "    if loss < bestloss:\n",
    "        bestloss = loss\n",
    "        bestW = W\n",
    "        bestB = b\n",
    "print(bestloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1156"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weight.set_data(bestW)\n",
    "net.bias.set_data(bestB)\n",
    "evaluate_accuracy(test_data, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chances are, we did a little better than guessing, but not by much. This isnt too surprising since our search is pretty much totally braindead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Method 2: Iterative Refinement\n",
    "Rather than try to find good weights all in one step randomly, let's try starting off with random weights, then making small incremental improvements. The idea here is that we're trying to take gradual steps to minimize the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blindfolded Hiker Analogy\n",
    "![hiker](https://i0.wp.com/www.slowbru.com/wp-content/gallery/feature-photos/P1090479c.JPG?w=1505)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you're stuck on a mountain and you can't see anything. You're goal is to get to the bottom of the mountain to find help. How might you proceed?\n",
    "\n",
    "You're best best is to feel the area around you and take a small step in the most downhill direction. Little step by little step, you'll work your way down to a lower point. Let's try implementing something like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.059775368073547\n",
      "12.063872955708504\n",
      "12.047404893531871\n",
      "12.079870734349408\n",
      "12.071828313856944\n",
      "12.06071943607698\n",
      "12.074631603229285\n",
      "12.080587752273487\n",
      "12.06819632284897\n",
      "12.058140230914713\n",
      "12.037845381003924\n",
      "12.0589133524021\n",
      "12.095497172233825\n",
      "12.114147287001197\n",
      "12.133503323581417\n",
      "12.132975141884002\n",
      "12.144219668126238\n",
      "12.176451792935913\n",
      "12.163096136879103\n",
      "12.172257966694376\n",
      "12.12590785873648\n",
      "12.117019643220301\n",
      "12.115369545439552\n",
      "12.118799829523615\n",
      "12.135190693686251\n",
      "12.15371385667314\n",
      "12.167807321086727\n",
      "12.196191918441752\n",
      "12.165933405781143\n",
      "12.16619690394417\n",
      "12.037845381003924\n"
     ]
    }
   ],
   "source": [
    "# keep track of the best accuracy we've seen\n",
    "bestloss = np.inf\n",
    "# also keep track of the best weights\n",
    "bestW = None\n",
    "bestB = None\n",
    "# try 1000 different weights\n",
    "for i in range(30):\n",
    "    step_size = 0.1\n",
    "    W = net.weight.data()\n",
    "    B = net.bias.data()\n",
    "    # generate a random weight tensor\n",
    "    Wtry = W + nd.random.normal(shape=(net.weight.shape)) * step_size\n",
    "    Btry = B + nd.random.normal(shape=net.bias.shape)*step_size\n",
    "    # assign those weights to our network\n",
    "    net.weight.set_data(Wtry)\n",
    "    net.bias.set_data(Btry)\n",
    "    # check the accuracy of the network\n",
    "    loss = evaluate_loss(test_data, net)\n",
    "    print(loss)\n",
    "    # see if this is the best we've done\n",
    "    if loss < bestloss:\n",
    "        bestloss = loss\n",
    "        bestW = Wtry\n",
    "        bestB = Btry\n",
    "    else:\n",
    "        # if its not better, go back\n",
    "        net.weight.set_data(W)\n",
    "        net.bias.set_data(B)\n",
    "print(bestloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1157"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weight.set_data(bestW)\n",
    "evaluate_accuracy(test_data, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as training proceeds, the loss gradually works its way down. This is a big improvement from taking random steps!\n",
    "\n",
    "However, right now we're trying steps in random directions, this is computationally inefficient. If you remember calculus, you'll note that we can simply use derivatives to determine which way to step!\n",
    "\n",
    "![slope](http://www.sosmath.com/calculus/diff/der00/der00_2.gif)\n",
    "\n",
    "![derivatives](http://hyperphysics.phy-astr.gsu.edu/hbase/Math/immath/derfunc.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our simple network. Remember that it's computation can be expressed as\n",
    "\n",
    "$Y' = WX$\n",
    "\n",
    "And the loss function is defined as\n",
    "\n",
    "$F({Y'}_i) = \\frac{e^{{Y'}_i}}{\\sum_{j=0}^{k} e^{{Y'}_j}}$\n",
    "\n",
    "$Loss = CE(Y', Y) = -\\sum_i log({F(Y')}_i) \\times Y_i$\n",
    "\n",
    "Our goal is to find the slope of the Loss with respect to our weights $W$, so that we can move $W$ in the appropriate direction. This goal can be expressed as\n",
    "\n",
    "$\\frac{dL}{dW}$\n",
    "\n",
    "This might seem like a very tough thing to compute, since there are already three equations in the way, but let's break up this complicated derivative into three parts using chain rule.\n",
    "\n",
    "$\\frac{dL}{dW} = \\frac{d{Y'}}{dW} \\times \\frac{dF}{d{Y'}} \\times \\frac{dL}{dF}$\n",
    "\n",
    "You can see that the right side cancels out to the left side, neat! This way, we can compute the derivative of each function seperate, then multiply them all together to get the full network derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets work on computing these derivatives one by one.\n",
    "\n",
    "$\\frac{d{Y'}}{dW} = \\frac{dWX}{dW} = X$\n",
    "\n",
    "This first one is pretty easy since it's just a multiplication.\n",
    "\n",
    "The next two get a little messy, and I'm going to avoid it since it'd be a slog to work through. There are plenty of online resources if you'd like to see it derived, but it turns out we can write\n",
    "\n",
    "$\\frac{dF}{d{Y'}} \\times \\frac{dL}{dF} = \\frac{dL}{d{Y'}} = F(Y') - Y$\n",
    "\n",
    "Thus, the derivative of our weights is equal to\n",
    "\n",
    "$X \\times F(Y') - Y$\n",
    "\n",
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of softmax cross entropy\n",
    "# X is output of dense layer\n",
    "# Y is labels\n",
    "def delta_cross_entropy(X, y):\n",
    "    # make label one hot for easier use\n",
    "    y = nd.one_hot(y, 10)\n",
    "    batch_size = y.shape[0]\n",
    "    grad = nd.softmax(X)\n",
    "    # subtract 1 (y) where appropriate\n",
    "    grad = grad - y\n",
    "    # scale by batch size\n",
    "    grad = grad / batch_size\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the gradient of matrix multiplication (dense layer).\n",
    "# gradin is the gradient of the output with respect to the loss\n",
    "# X is the input to the layer\n",
    "def delta_matrix_mul(gradin, X):\n",
    "    W_updates = nd.dot(gradin.transpose(), X)\n",
    "    return W_updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats not so bad! Let's try using these new tricks to do training!\n",
    "\n",
    "Now that we know how the loss relates to the weights, we know which direction to move the weights in! Specifically, we want the loss to decrease, so we move the weights in the opposite direction of the derivative. The update for one step of training is thus\n",
    "\n",
    "$W = W + lr \\times \\frac{dW}{dL} = W + lr \\times (X (F(Y') - Y))$\n",
    "\n",
    "Where lr is the learning rate. Basically, the learning rate represents how big a step we should take. Just like in blind-folded hiking, it's usually safer to take small steps rather than big ones.\n",
    "\n",
    "Now, we implement our custom gradient descent in a training loop, it'll likely look similar, but note that we dont use autograd at all. The mxnet autograd package actually stands for automatic gradients, but here we're clearly computing our own gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = .1\n",
    "\n",
    "def custom_train(net, epochs=10):\n",
    "    for e in range(epochs):\n",
    "        # we're going to sum up the loss over the whole epoch\n",
    "        cumulative_loss = 0\n",
    "        # iterate through all the training data\n",
    "        for i, (data, label) in enumerate(train_data):\n",
    "            # make sure the data is on the right device, flatten the images\n",
    "            data = data.as_in_context(model_ctx).reshape((-1,784))\n",
    "            label = label.as_in_context(model_ctx)\n",
    "            # update our loss for this epoch\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "            # keep track of loss for printing purposes\n",
    "            cumulative_loss += nd.sum(loss).asscalar()\n",
    "            # now lets apply our custom gradients!\n",
    "            dFdL = delta_cross_entropy(output, label)\n",
    "            dWdF = delta_matrix_mul(dFdL, data)\n",
    "            # derivative of bias is just 1\n",
    "            dBdF = nd.mean(dFdL, axis=0)\n",
    "            # use our weight gradients to update the weights and bias\n",
    "            W = net.weight.data()\n",
    "            B = net.bias.data()\n",
    "            # update the weight and bias\n",
    "            new_W = W - (learning_rate * dWdF)\n",
    "            new_B = B - (learning_rate * dBdF)\n",
    "            net.weight.set_data(new_W)\n",
    "            net.bias.set_data(new_B)\n",
    "        print(\"Epoch %s. Loss: %s\" % (e, cumulative_loss/num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.2775573268731435\n",
      "Epoch 1. Loss: 0.27582611340284346\n",
      "Epoch 2. Loss: 0.27451042377154034\n",
      "Epoch 3. Loss: 0.27331182289123535\n",
      "Epoch 4. Loss: 0.2723051885843277\n",
      "Epoch 5. Loss: 0.2710574910879135\n",
      "Epoch 6. Loss: 0.27040566515525183\n",
      "Epoch 7. Loss: 0.26928829301198326\n",
      "Epoch 8. Loss: 0.26815170016686124\n",
      "Epoch 9. Loss: 0.26732689545551935\n"
     ]
    }
   ],
   "source": [
    "custom_train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9194"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(test_data, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked much better! The difference is pretty much night and day. Although the random step method should in theory get similar results, it would take an absurd amount of time to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradients for Large Networks: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![simplebackprop](http://jwfromm.com/GIX513/images/simple_backprop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a different way of looking at what we just did. We have an input X to a dense layer, which uses it's weights to compute Y. If we rename the dense layer as $F(X)$, then just as before want to compute.\n",
    "\n",
    "$\\frac{dW}{dY}$\n",
    "\n",
    "Where Y could be considered the loss in this case. We know how to do this of course. But what if it were a more complicated network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![medium](http://jwfromm.com/GIX513/images/medium_backprop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have two dense layers, $F1$ and $F2$. Let's take a look at how we could compute the weight updates for both!\n",
    "\n",
    "In this case, the loss is represented by the output node $Y2$, so thats what we'll compute our derivatives with respect to. Since $W2$ is closer to $Y2$ than $W1$ is, we'll start by finding it's gradient.\n",
    "\n",
    "$\\frac{dW_2}{dY_2} = Y_1$\n",
    "\n",
    "Well that was easy, this is basically the same equation as when we were doing a single layer. Now lets move on to finding the gradient of $W_1$.\n",
    "\n",
    "$\\frac{dW_1}{dY_2} = ....$\n",
    "\n",
    "hmm, theres a bunch of stuff in the way, the relationship between $W_1$ and $Y_2$ isn't directly obvious. Let's start by looking at nodes close to $W.1$ though.\n",
    "\n",
    "$\\frac{dW_1}{dY_1} = X$\n",
    "\n",
    "This looks familiar! Now that we have the gradient with respect to $Y_1$, we can try to find $\\frac{dY_1}{dY_2}$.\n",
    "\n",
    "$\\frac{dY_1}{dY_2} = W_2$\n",
    "\n",
    "This looks similar to our dense layer, but the gradient is defined by the weight instead of the input. That's because we took the derivative of an input with respect to an output instead of a weight with respect to an output. Now we can solve for $W_1$'s full gradient.\n",
    "\n",
    "$\\frac{dW_1}{dY_2} = \\frac{dW_1}{dY_1} \\times \\frac{dY_1}{dY_2} = X \\times W_2$\n",
    "\n",
    "And thats it! Now we have a two layer gradient. But what if its even more complicated? Let's try looking at this in a different way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![medium](http://jwfromm.com/GIX513/images/full_backprop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image above, we try to compute the full gradient for each weight in a 3 layer network. You might notice there are now backwards arrows, that's because we're going to allow the gradients to flow from the loss back to the weights.\n",
    "\n",
    "You can see in the bottom dense layer the weight update is simply\n",
    "\n",
    "$\\frac{dW_3}{dY_3}$\n",
    "\n",
    "Which is quite easy to compute.\n",
    "\n",
    "We also know how to compute $\\frac{dY_2}{dY_3}$\n",
    "\n",
    "Next, we move up a layer. To update $W_2$ we must compute:\n",
    "\n",
    "$\\frac{dW_2}{dY_3} = \\frac{dY_2}{dY_3} \\times \\frac{dW_2}{dY_2}$\n",
    "\n",
    "But we've already computed $\\frac{dY_2}{dY_3}$! So we need only additionally compute $\\frac{dW_2}{dY_2}$. Fortunately, this single gradient is also very easy to compute!\n",
    "\n",
    "We similary find $\\frac{dY_1}{dY_3} = \\frac{dY_2}{dY_3} * \\frac{dY_1}{dY_2}$\n",
    "\n",
    "Finally, we move up a layer to find $\\frac{dW_1}{dY_3}$. This is expressed by\n",
    "\n",
    "$\\frac{dW_1}{dY_3} = \\frac{dY_2}{dY_3} \\times \\frac{dY_1}{dY_2} \\times \\frac{dW_1}{dY_1}$\n",
    "\n",
    "This is starting to get quite large, but we just found $\\frac{dY_2}{dY_3} \\times \\frac{dY_1}{dY_2}$, so we need only multiply that by our current simple gradient $\\frac{dW_1}{dY_1}$! This is actually quite simple since we allow the gradients to accumulate as they propagate up the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that no matter how complex your network is, you can make a graph similar to above and as long as each function node is differentiable, you can propagate the gradient backwards up the net to find each parameter gradient. This is a surprisingly efficient operation that is all thanks to chain-rule in calculus. \n",
    "\n",
    "This is exactly the computation that mxnet performs when you say\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
