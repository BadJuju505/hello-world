{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Neural Networks\n",
    "This lecture will introduce multi-layer neural networks and work through several examples using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard mxnet packages\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "\n",
    "# import matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, there were a few questions about how everything we've learned so far fits into the grand scheme. Specifically, people were having trouble relating KNN and perceptron to other algorithms we've discussed. The question of \"why are we talking about these things?\" shows a misunderstanding of what machine learning is trying to do.\n",
    "\n",
    "All machine learning can be structured in the following way:\n",
    "\n",
    "![mlchart](http://jwfromm.com/GIX513/images/ML_chart.png)\n",
    "\n",
    "Basically, we have some set of input features and a correspoding output or label. A function maps a set of inputs to an output, so there must be some function that maps our input features to the output/label. The goal of machine learning is to learn what that function is.\n",
    "\n",
    "In easy cases, the function may be quite simple, like a line. However, for harder problems, the function to be learned can be very complex. The diversity in difficulty of problems is why the field of machine learning has so many different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the techniques we've discussed, and all the techniques and algorithms we will discuss, are just different tools for learning F(x).\n",
    "\n",
    "![toolschart](http://jwfromm.com/GIX513/images/Tools_chart.png)\n",
    "\n",
    "We've started out with some of the simplest tools, KNN and perceptron. To be perfectly clear, these will work well for some problems and are completely viable tools that show up in industry. In fact, it's much better to use these simple tools when they work than more advanced tools, since the simple tools are cheaper.\n",
    "\n",
    "However, for some problems, you may need a more sophisticated method for learning F(x). Today, we will start covering the most famous of the more sophisticated tools.\n",
    "\n",
    "Always remember that each different algorithm we learn about is one of many tools. The most important part in understanding machine learning is figuring out which tool to use for a problem. The goal of this course is to fill your toolbox with enough options that you can make a good choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "The perceptron that we discussed last time is a neural network! Specifically, it is a single neuron.\n",
    "\n",
    "![perceptron](https://cdn-images-1.medium.com/max/1600/1*n6sJ4yZQzwKL9wnF5wnVNg.png)\n",
    "\n",
    "It may seem that improving the perceptron would be as simple as adding more neurons. Let's try sticking two perceptrons together and see what we get!\n",
    "\n",
    "![2perceptrons](http://jwfromm.com/GIX513/images/2_perceptrons.png)\n",
    "\n",
    "Let's try writing the variables in terms of the inputs.\n",
    "\n",
    "$y_1 = w_1 \\cdot x + b_1$\n",
    "\n",
    "$y_2 = w_2 \\cdot y_1 + b_2$\n",
    "\n",
    "Let's now substitute the first equation into the second\n",
    "\n",
    "$y_2 = w_2 \\cdot (w_1 \\cdot x + b_1) + b_2$\n",
    "\n",
    "Now, lets simplify a little\n",
    "\n",
    "$y_2 = w_2 \\cdot w_1 \\cdot x + (w_2 \\cdot b_1 + b_2)$\n",
    "\n",
    "Interesting, what if we define some new variables?\n",
    "\n",
    "$w_3 = w_2 \\cdot w_1$\n",
    "\n",
    "$b_3 = w_2 \\cdot b_1 + b_2$\n",
    "\n",
    "Let's substitute these new variables back in to our equation for $y_2$\n",
    "\n",
    "$y_2 = w_3 \\cdot x + b_3$\n",
    "\n",
    "Hmm, this looks a lot like the equation for a single perceptron, in fact it is identical to it.\n",
    "\n",
    "![combo](http://jwfromm.com/GIX513/images/combo_perceptrons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our combination of two perceptrons is exactly the same as some other single perceptron! What's the point of putting two together if another perceptron does just as well?\n",
    "\n",
    "The answer is that there isn't a point. Because each neuron of a perceptron is perfectly linear, combining multiple together does not provide additional information. We need to do something to make a neuron non-linear, that way the combination of many neurons __will__ give us extra information.\n",
    "\n",
    "![nonlin](https://i.stack.imgur.com/ibYr3.png)\n",
    "\n",
    "The solution is to add what's called an activation to the output of the neuron. This performs some __non linear__ operation on the output before being fed to the next neuron.\n",
    "\n",
    "There are actually quite a few options for activations, lets take a look at a few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU Activation\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "x = np.arange(-1, 1, .1)\n",
    "plt.plot(x, relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sigmoid activation\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.arange(-5, 5, .1)\n",
    "plt.plot(x, sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tanh Activation\n",
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "x = np.arange(-5, 5, .1)\n",
    "plt.plot(x, tanh(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softrelu activation\n",
    "def softrelu(x):\n",
    "    return np.log(1+np.exp(x))\n",
    "\n",
    "x = np.arange(-5, 5, .1)\n",
    "plt.plot(x, softrelu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take another look at the two neuron case, but add activations.\n",
    "\n",
    "![combo](http://jwfromm.com/GIX513/images/relu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, lets try to express the various outputs algebraically\n",
    "\n",
    "$y_1 = \\text{relu}(w_1 \\cdot x + b_1)$\n",
    "\n",
    "$y_2 = \\text{relu}(w_2 \\cdot y_1 + b_2)$\n",
    "\n",
    "Now, just as before, lets try to substitute in $y_1$ to the equation for $y_2$\n",
    "\n",
    "$y_2 = \\text{relu}(w_2 \\cdot (\\text{relu}(w_1 \\cdot x + b_1)) + b_2)$\n",
    "\n",
    "Hmm, because relu is non-linear, I can't really simplify this anymore. It's clear that there's no way i could pick a $w_3$ and $b_3$ for a single new neuron that would give me the same output. Thus, having two neurons actually is giving us more information!\n",
    "\n",
    "That's really big news, now we can combine tons of neurons and get more processing power!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be blunt, the more neurons you add to a network, the more processing power it gets. However, it also becomes more difficult to train, so don't go too crazy.\n",
    "\n",
    "![neural](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's talk about the steps involved in training a neural network.\n",
    "\n",
    "![flowchart](http://jwfromm.com/GIX513/images/flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a few terms that you'll be seeing a lot.\n",
    "\n",
    "* __Transform__: Some sort of function applied to the input data, for example resizing images to a fixed dimension\n",
    "* __Minibatch__: A subset of data that is looked at together. Used when there is too much input data to process all at once.\n",
    "* __DataLoader__: An mxnet utility that help feed data to a neural network\n",
    "* __Dense__: Another name for a layer of neurons\n",
    "* __Trainer__: The learning algorithm used (SGD)\n",
    "* __Loss__: How far off our guess was from the right label\n",
    "* __Predictions__: The output guess of a network\n",
    "* __Backwards__: Compute the gradient for all parameters of a network\n",
    "* __Epoch__: Iterating through all the inputs once\n",
    "* __Step__: Process a single minibatch and update parameters once\n",
    "* __Learning Rate__: The speed that the network learns, too fast and it might not learn well, too slow and you'll get tired waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device we should use for computing, we'll just our cpu for now\n",
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST, a very simple image classification dataset\n",
    "\n",
    "# set the size of the training set\n",
    "num_examples = 60000\n",
    "# set batch size : how many images should i process at a time?\n",
    "batch_size = 64\n",
    "# set the number of pixels per image (32 x 32)\n",
    "num_inputs = 784\n",
    "# set the number of possible outputs (0 through 9)\n",
    "num_outputs = 10\n",
    "# define a function that scales the image pixels down between 0 and 1\n",
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "# load the training data\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "# load the test data\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's sample 5 random data points from the test set\n",
    "sample_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                              1, shuffle=True)\n",
    "for i, (data, label) in enumerate(sample_data):\n",
    "    data = data.reshape(data.shape[1:-1])\n",
    "    plt.imshow(data.asnumpy())\n",
    "    plt.show()\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a minimal neural network\n",
    "net = gluon.nn.Dense(num_outputs)\n",
    "# initialize the parameters of the network randomly\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![simplenet](http://jwfromm.com/GIX513/images/simplenet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function, for virtually all classification problems we will use softmax-crossentropy loss.\n",
    "# this is a combination of softmax function, which squished things down to a probability between 0 and 100%, \n",
    "# and cross entropy loss, which measure how close we are to the correct label.\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an optimizer, let's use basic stochastic gradient descent\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to see how good our model is\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    # keep track of the accuracy across our dataset\n",
    "    acc = mx.metric.Accuracy()\n",
    "    # iterate through all the data\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        # move the data and label to the proper device\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        # run the data through the network\n",
    "        output = net(data)\n",
    "        # check what our guess is\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        # compute accuracy and update our running tally\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    # return the accuracy\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets go ahead and try running our test data through our network. We havent trained anything yet, so we expect \n",
    "# low accuracy.\n",
    "evaluate_accuracy(test_data, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's get to the good part, training the network!\n",
    "\n",
    "def train(net, trainer, epochs=3):\n",
    "    # iterate through the epochs\n",
    "    loss_history = []\n",
    "    for e in range(epochs):\n",
    "        # we're going to sum up the loss over the whole epoch\n",
    "        cumulative_loss = 0\n",
    "        # iterate through all the training data\n",
    "        for i, (data, label) in enumerate(train_data):\n",
    "            # make sure the data is on the right device, flatten the images\n",
    "            data = data.as_in_context(model_ctx).reshape((-1,784))\n",
    "            label = label.as_in_context(model_ctx)\n",
    "            # compute the output and loss while keeping track of gradients\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            # calculate all the derivatives with respect to the loss\n",
    "            loss.backward()\n",
    "            # update weights based on the derivative\n",
    "            trainer.step(batch_size)\n",
    "            # update our loss for this epoch\n",
    "            cumulative_loss += nd.sum(loss).asscalar()\n",
    "\n",
    "        print(\"Epoch %s. Loss: %s\" % (e, cumulative_loss/num_examples))\n",
    "        loss_history.append(cumulative_loss/num_examples)\n",
    "    return np.arange(epochs), loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train(net, trainer)\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets check the accuracies again\n",
    "test_accuracy = evaluate_accuracy(test_data, net)\n",
    "train_accuracy = evaluate_accuracy(train_data, net)\n",
    "print(test_accuracy)\n",
    "print(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(net,data):\n",
    "    output = net(data.as_in_context(model_ctx))\n",
    "    return nd.argmax(output, axis=1)\n",
    "\n",
    "# let's sample 10 random data points from the test set\n",
    "sample_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                              10, shuffle=True)\n",
    "for i, (data, label) in enumerate(sample_data):\n",
    "    data = data.as_in_context(model_ctx)\n",
    "    print(data.shape)\n",
    "    im = nd.transpose(data,(1,0,2,3))\n",
    "    im = nd.reshape(im,(28,10*28,1))\n",
    "    imtiles = nd.tile(im, (1,1,3))\n",
    "    plt.imshow(imtiles.asnumpy())\n",
    "    plt.show()\n",
    "    pred=model_predict(net,data.reshape((-1,784)))\n",
    "    print('model predictions are:', pred)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat! We just trained our first neural network. And what's even better, it got pretty high accuracy! This is especially impressive since our network is literally as simple as it could be while still working at all. Let's see if we can make a spicier net by adding some extra layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the gluon sequential class to build up a multilayer neural network\n",
    "spicynet = gluon.nn.HybridSequential()\n",
    "with spicynet.name_scope():\n",
    "    spicynet.add(gluon.nn.Dense(128, activation='relu'))\n",
    "    spicynet.add(gluon.nn.Dense(64, activation='relu'))\n",
    "    spicynet.add(gluon.nn.Dense(num_outputs))\n",
    "spicynet.hybridize()\n",
    "spicynet.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=model_ctx)\n",
    "spicy_trainer = gluon.Trainer(spicynet.collect_params(), 'sgd', {'learning_rate': .1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spicynet](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mlp_mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Now we have a 3 layer network that is much more sophisticated than before. We can train and test it in exactly the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train(spicynet, spicy_trainer, epochs=3)\n",
    "plt.plot(x, y)\n",
    "evaluate_accuracy(test_data, spicynet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
